{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import zip_longest, chain\n",
    "from time import sleep\n",
    "import snoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c77141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url grabber\n",
    "def url_topic_grabber(url,start,end):\n",
    "    urls = []\n",
    "    for i in range(start,end):\n",
    "        url_ = f'{url}p{i}'\n",
    "        request = requests.get(url_).text\n",
    "        soup = bs(request,'lxml')\n",
    "        href = soup.find_all('a',class_ = 'text-black block')\n",
    "        for i in href:\n",
    "            urls.append(i.get_attribute_list('href'))\n",
    "    urls = set(list(chain.from_iterable(urls)))\n",
    "    return list(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35835a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@snoop\n",
    "def grab_details(urls, topic):\n",
    "    day = []\n",
    "    month = []\n",
    "    year = []\n",
    "    category = []\n",
    "    article_ = []\n",
    "    kurz_text = []\n",
    "    haupt_text = []\n",
    "    \n",
    "    count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            request = requests.get(url).text\n",
    "            soup = bs(request, 'lxml')\n",
    "            article_.append(soup.find('span',class_='align-middle').text)\n",
    "            kurz_text.append(soup.find('div',class_=\"RichText RichText--sans leading-loose lg:text-xl md:text-xl sm:text-l lg:mb-32 md:mb-32 sm:mb-24\").text)\n",
    "            haupt_text.append(soup.find('div',class_=\"RichText RichText--iconLinks lg:w-8/12 md:w-10/12 lg:mx-auto md:mx-auto lg:px-24 md:px-24 sm:px-16 break-words word-wrap\").text)\n",
    "            day.append(int(soup.find('time', class_='timeformat').text.split(',')[0].split('.')[0]))\n",
    "            month.append(int(soup.find('time', class_='timeformat').text.split(',')[0].split('.')[1]))\n",
    "            year.append(int(soup.find('time', class_='timeformat').text.split(',')[0].split('.')[2]))\n",
    "            count +=1\n",
    "            if count % 100 == 0:\n",
    "                print(f'{count} pages done!')\n",
    "            category.append(topic)\n",
    "        except:\n",
    "            pass\n",
    "    print(f'last page done! {count}')\n",
    "    \n",
    "    list_of_results = [day, month, year, category, article_, kurz_text, haupt_text]\n",
    "    export_data = zip_longest(*list_of_results, fillvalue = '')\n",
    "    df = pd.DataFrame(export_data,columns=['day', 'month', 'year', 'category', 'article', 'kurz_text', 'haupt_text'])\n",
    "    df.to_csv(f'{topic}.csv', index = True, index_label='Line_ID', header = True )\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f129f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the urls\n",
    "politik_urls = url_topic_grabber('https://www.spiegel.de/politik/',1,101)\n",
    "sport_urls = url_topic_grabber('https://www.spiegel.de/sport/',1,101)\n",
    "kultur_urls = url_topic_grabber('https://www.spiegel.de/kultur/',1,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function through all three url list\n",
    "\n",
    "politik = grab_details(politik_urls,'Politik')\n",
    "sport = grab_details(sport_urls,'Sport')\n",
    "kultur = grab_details(kultur_urls,'Kultur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f0c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f3dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_results = [day, month, year, category, article_, kurz_text, haupt_text]\n",
    "export_data = zip_longest(*list_of_results, fillvalue = '')\n",
    "\n",
    "df = pd.DataFrame(export_data,columns=['day', 'month', 'year', 'category', 'article', 'kurz_text', 'haupt_text'])\n",
    "df.to_csv(f'{}.csv', index = True, index_label='Line_ID', header = True )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
