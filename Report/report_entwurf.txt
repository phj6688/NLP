siehe overleaf f√ºr template

1 Introduction

The concept of simple language [cit. Simple language ^1] is the solution
for creating accessible text for people with lower capabilities
of reading complex texts. Its used by news sites to provide
valuable information to people regardless of their literacy[cit. nachrichtenleicht.de].

In this work we look at German simple language, or "Leichte Sprache". We are examining
the implications simple language has on natural language processing. Simple language
is distinct from conventional text, especially so in news context, where the standard
of text usually is fairly high. Simple language aims to maintain high readibility even
on complex topics.

For this we focus on two main experiments. One, do the common features used when classifying
news articles work at a comparable level when applied to simple language, or can we tune the features to
obtain better results in classifying it. And two, is it worth it to train a classifier on simple
language specifically, if we intend to use it for simple language, or does training on conventional
text give a similar result.

The motivation for this is to figure out if there is a benefit in considering simple language
when designing a classifier, or if it is close enough to conventional language to simply treat it as such.


2 Data
We are using datasets we gathered ourselves. We use news articles, as there is plenty of available
data in simple language, and it offers itself nicely for labelling and classifying,
by using the general category as a label. We take our data from nachrichtenleicht.de [cit.] for simple
language and from spiegel.de [cit.] for our conventional language.
For our supervised training we require labels, but we are able to easily obtain them, as they are provided
already, since each article is categorized by the source under "culture", "sport", "news" or for our purposes
"politics" and "miscellaneous". We will focus on the former three categories, as that gives us a more clear
distinction for each label.

We cleaned the data, formatting each article into a single line, with the document consisting of
the headline, followed by a short description of the article, and then the body of the article itself.
Aftet cleaning we gathered some metadata, to give a better overview of our dataset, and to be able
to judge some of the results accordingly. The result can be seen in [table 1]


Tabelle: siehe overleaf

Two aspects to point out are the clear and significant difference in the reading ease, indicating
that we chose data that is far apart in its style and language level, and is thus fit for researching
our questions. The other point to consider is the comparable size of the labels, meaning we do not
need to pay too much attention to possible problems arising from unbalanced data.





3 Approach

3.1 General
[in case of unbalanced data, mention is here]
As the volume of data for the simple language is quite small,
we will be using cross validation for testing. In case we intend to tune our model,
we hold out 10% of the data as final testing data after tuning.
We use a k [decide how many] fold standard cross validation.

We observe the standard metrics for our models, accuracy, precision and recall, and the f1 score.
We use these standard metrics since our goal is not to determine the actual quality of our models,
but to compare them against one another. Therefore having a broader overview of general performance
should be enough to indicate a trend.

3.2 Which features are the best

	In this experiment we will use ___ classifier. Since we intend to tune our models,
	we hold out a random sample of 10% of our data.
	
	- Which standard features are we comparing
	- How are we adapting these features with custom features
	- Are we using entirely new custom ideas for features?
		- using readability as a feature

3.3 Is it worth to train on simple


4 Results

4.1 features

4.2 worth it to train


5 Conclusion

6 Outlook and discussion






^1 source for what simple language is